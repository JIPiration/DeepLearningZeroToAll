{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Tutorial: Bird by Bird using Deep Learning</h1>\n",
    "<h2 align=\"center\">Advancing deep learning models for fine-grained classification of bird species</h2>\n",
    "<h3 align=\"center\">Author: Sofya Lipnitskaya</h3>\n",
    "\n",
    "### This repository related to [Bird by Bird using Deep Learning](https://github.com/slipnitskaya/caltech-birds-advanced-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "With this tutorial, you will tackle such an established problem in computer vision as fine-grained classification of bird species. The notebook demonstrates how to classify bird images from the Caltech-UCSD Birds-200-2011 ([CUB-200-2011](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)) dataset using PyTorch, one of the most popular open-source frameworks for deep learning experiments. \n",
    "\n",
    "**Project outline** \n",
    "\n",
    "Here you can get familiarized with the content more properly (the respective CRISP-DM stages of the project are indicated in parentheses):\n",
    "\n",
    "* Introducing the bird species recognition problem [(Business Understanding)](#motiv)\n",
    "* Exploratory analysis of CUB-200-2011 dataset [(Data Understanding)](#data)\n",
    "* Transforming images and splitting the data [(Data Preparation)](#prep) \n",
    "* Training and evaluation of the baseline model [(Modelling. p. 1/2)](#model-base)\n",
    "* Advancing the deep learning model [(Modelling. p. 2/2)](#model-adv) \n",
    "* Conclusions and Future work [(Evaluation)](#eval)\n",
    "\n",
    "**Learning Goals**\n",
    "\n",
    "By the end of the tutorial, you will be able to:\n",
    "- Understand basics of image classification problem of bird species.\n",
    "- Determine the data-driven image pre-processing strategy.\n",
    "- Create your own deep learning pipeline for image classification.\n",
    "- Build, train and evaluate ResNet-50 model to predict bird species.\n",
    "- Improve the model performance by using different techniques.\n",
    "\n",
    "***\n",
    "\n",
    "## Introducing the bird species recognition problem<a class=\"anchor\" id=\"motiv\"></a>\n",
    "\n",
    "**Motivation** \n",
    "\n",
    "Bird species recognition is a difficult task challenging the visual abilities for both human experts and computers. One of the interesting task related to that problem implies the classification of birds by species using imagery data collected from aerial surveys. Bird populations are important biodiversity indicators, so collecting reliable data is quite [important](https://dl.acm.org/doi/10.1016/j.patrec.2015.08.015) to ecologists. Recognition of bird species also benefits companies developing wind farms producing renewable energy, since their construction requires the prior risk assessment of bird collisions, threatening many of the world’s species with extinction.\n",
    "\n",
    "This, of course, would be a very ambitious plan to try to find the solution for this problem within a single notebook, so let's make it simple and focus on the bird classification. To make it even more concise, here, we are going to create and evaluate a deep learning model to classify bird images from the Caltech-UCSD Birds-200-2011 ([CUB-200-2011](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)) dataset. In this tutorial, you will learn how to perform the data-driven image pre-processing, build a baseline ResNet-based classifier, and to improve its performance for even better results in bird species recognition using different techniques, which will be described later on.\n",
    "\n",
    "**Questions to be solved through the notebook:**\n",
    "\n",
    "1. Do corrupted images exist in our dataset?\n",
    "2. What would be the optimal data transformation strategy?\n",
    "3. Are there any image-specific biases that can limit the model performance?\n",
    "4. How to handle overfitting given the limited amount of training samples?\n",
    "5. How to improve the model performance in bird species recognition?\n",
    "\n",
    "***\n",
    "\n",
    "First, let's import packages that we will use in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "#import targfile\n",
    "import multiprocessing as mp\n",
    "\n",
    "import tqdm\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.model_selection as skms\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as td\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision as tv\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# define constants\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "OUT_DIR = 'results'\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# create an output  folder\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    try:\n",
    "        os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    except OSError as exc:\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise Exception('OSError')\n",
    "\n",
    "            \n",
    "def get_model_desc(pretrained=False, num_classes=200, use_attention=False):\n",
    "    \"\"\"\n",
    "    Generates description string.\n",
    "    \"\"\"\n",
    "    desc = []\n",
    "    \n",
    "    if pretrained:\n",
    "        desc.append('Transfer')\n",
    "    else:\n",
    "        desc.append('Baseline')\n",
    "        \n",
    "    if num_classes == 204:\n",
    "        desc.append('Multitask')\n",
    "\n",
    "    if use_attention:\n",
    "        desc.append('Attention')\n",
    "    \n",
    "    return '-'.join(desc)\n",
    "\n",
    "\n",
    "def log_accuracy(path_to_csv, desc, acc, sep='\\t', newline='\\n'):\n",
    "    \"\"\"\n",
    "    Logs accuracy into a CSV-file\n",
    "    \"\"\"\n",
    "    file_exists = os.path_exists(path_to_csv)\n",
    "    \n",
    "    mode = 'a'\n",
    "    if not file_exists:\n",
    "        mode += '+'\n",
    "    \n",
    "    with open(path_to_csv, mode) as csv:\n",
    "        if not file_exists:\n",
    "            csv.write(f'setup{sep}accuracy{newline}')\n",
    "        \n",
    "        csv.write(f'{desc}{sep}{acc}{newline}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection<a class=\"anchor\" id=\"data\"></a>\n",
    "\n",
    "In this tutorial, we are going to use CUB-200-2011 dataset consisting of 11788 images of birds belonging to 200 species. \n",
    "\n",
    "The dataset file can be downloaded and extracted manually from [link](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html 'www.vision.caltech.edu'), or, alternatively, using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleDriveDownloader(object):\n",
    "    \"\"\"\n",
    "    Downloading a file stored on Google Drive by its URL.\n",
    "    If the link is pointing to another resource, the redirect chain is being expanded.\n",
    "    \n",
    "    Return the output path.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_url = 'https://docs.google.com/uc?export=download'\n",
    "    chunk_size = 32768\n",
    "    \n",
    "    def __init__(self, url, out_dir):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.out_name = url.rsplit('/', 1)[-1]\n",
    "        self.url = self._get_redirect_url(url)\n",
    "        self.out_dir = out_dir\n",
    "        \n",
    "    @staticmethod\n",
    "    def _get_redirect_url(url):\n",
    "        response = requests.get(url)\n",
    "        if response.url != url and response.url is not None:\n",
    "            redirect_url = response.url\n",
    "            return redirect_url\n",
    "        else:\n",
    "            return url\n",
    "        \n",
    "    @staticmethod\n",
    "    def _get_confirm_token(response):\n",
    "        for key, value in response.cookies.items():\n",
    "            if key.startswith('download_warning'):\n",
    "                return value\n",
    "        return None\n",
    "    \n",
    "    def _save_response_content(self, response):\n",
    "        with open(self.fpath, 'wb') as f:\n",
    "            bar = tqdm.tqdm(total=None)\n",
    "            progress = 0\n",
    "            for chunk in response.iter_content(self.chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    progress += len(chunk)\n",
    "                    bar.update(progress - bar.n)\n",
    "            bar.close()\n",
    "    \n",
    "    @property\n",
    "    def file_id(self):\n",
    "        return self.url.split('?')[0].split('/')[-2]\n",
    "    \n",
    "    @property\n",
    "    def fpath(self):\n",
    "        return os.path.join(self.out_dir, self.out_name)\n",
    "    \n",
    "    def download(self):\n",
    "        os.makedirs(self.out_dir, exist_ok=True)\n",
    "        \n",
    "        if os.path.isfile(self.fpath):\n",
    "            print(\"File is downloaded yet: \", self.fpath)\n",
    "        else:\n",
    "            session = requests.Session()\n",
    "            response = session.get(self.base_url, params={'id': self.file_id}, stream=True)\n",
    "            token = self._get_confirm_token(response)\n",
    "            \n",
    "            if token:\n",
    "                response = session.get(self.base_url, params={'id': self.file_id, 'confirm': token}, stream=True)\n",
    "            else:\n",
    "                raise RuntimeError()\n",
    "                \n",
    "            self._save_response_content(response)\n",
    "        \n",
    "        return self.path\n",
    "    \n",
    "# download an archive containing the dataset and store it into the output directory\n",
    "url = 'http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz'\n",
    "dl = GoogleDriveDownloader(url, 'data')\n",
    "dl.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading of the compressed file, we extract it and also assess some statistics to verify whether the gathered data consist the expected number of classes and images. Here's an example execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tgz(from_path, to_path=None, img_extention='.jpg'):\n",
    "    \"\"\"\n",
    "    Extracts data from '.tgz' file and displays data statistics.\n",
    "    Returns the output directory name.  \n",
    "    \"\"\"\n",
    "    with tarfile.open(from_path, 'r:gz') as tar:   \n",
    "        \n",
    "        if to_path is None:\n",
    "            out_dir = os.path.splitext(from_path)[0]\n",
    "        if os.path.isdir(out_dir):\n",
    "            print('Files are extracted yet.')\n",
    "        else:\n",
    "            print('Extracting files...')\n",
    "        to_path = os.path.dirname(out_dir)\n",
    "\n",
    "        subdir_and_files = [tarinfo for tarinfo in tar.getmembers()]    \n",
    "        imgs = [t for t in subdir_and_files if t.name.endswith(img_extention)]\n",
    "        print('\\tClasses: {}\\n\\tImages: {}'.format(len(set([os.path.dirname(t.name) for t in imgs])), len(imgs)))\n",
    "\n",
    "        tar.extractall(to_path, members=subdir_and_files)\n",
    "        \n",
    "        return out_dir\n",
    "\n",
    "\n",
    "# extract the downloaded archive & assess data statistics\n",
    "in_dir_data = extract_tgz(from_path=dl.fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Exploratory data analysis is an essential starting point of any data science project, which provides a better data understanding and lays the foundation for the further analysis. First, we are going to make a quality check to ensure files are correctly collected and verify whether the data cleaning will be needed: \n",
    "\n",
    "* Question 1: Do corrupted images exist in our dataset?\n",
    "\n",
    "In this tutorial, we plan to evaluate several network's architectures, including the one which are pre-trained using the ImageNet dataset. Since pre-trained models usually expect input data to be normalized in the same way, such as a height and width are at least 224 pixels, we can start thinking about:\n",
    "\n",
    "* Question 2: What would be the optimal data transformation strategy?\n",
    "\n",
    "Additionally, we plan to estimate the position on which birds more likely to be located on the image. This he solution will be helpful to define the appropriate way for the data augmentation later on. For that, we plan to investigate: \n",
    "\n",
    "* Question 3: Are there any image-specific biases that can limit the model performance?\n",
    "\n",
    "Let's start exploring data from the CUB-200-2011 dataset.  \n",
    "\n",
    "### Question 1: Do corrupted images exist in our dataset?\n",
    "\n",
    "Data cleaning is the process of ensuring data is correct, so we want to assess whether downloaded dataset contains corrupted images. For that purpose, we'll calculate standard deviation among all pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths(path_to_data, fileformat='.jpg'):\n",
    "    \"\"\"\n",
    "    Returns patgs to files of the specified format.\n",
    "    \"\"\"\n",
    "    filepaths = []\n",
    "    for root, _, filenames in os.walk(path_to_data):\n",
    "        for fn in filenames:\n",
    "            if fn.endswith(fileformat):\n",
    "                filepaths.append(os.path.join(root, fn))\n",
    "                \n",
    "    return filepaths\n",
    "\n",
    "def clearning_worker(path_to_img):\n",
    "    \"\"\"\n",
    "    Verifies whether the image in corrupted.\n",
    "    \"\"\"\n",
    "    std = np.std(mpimg.imread(path_to_img))\n",
    "    img_ok = not np.isclose(std, 0.0)\n",
    "    \n",
    "    return img_ok, path_to_img\n",
    "\n",
    "# calculate standard deviation of images\n",
    "#  multiprocessing(mp)\n",
    "imgs_corrupted = []\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    for img_ok, fn in pool.imap_unordered(cleaning_worker, get_filepaths(in_dir_img)):\n",
    "        if not img_ok:\n",
    "            imgs_corrupted.append(fn)\n",
    "\n",
    "# verify do corrupted images (missing data) exist\n",
    "print('Corrupted images #: ', len(imgs_corrupted))\n",
    "\n",
    "# clean up the images that arent' OK\n",
    "for fn in imgs_corrupted:\n",
    "    os.remove(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: What would be an optimal image transformation strategy?\n",
    "\n",
    "In order to define the optimal data transformation strategy, we'll explore the format of images to see what useful we can grasp on. Let’s have a look at some bird examples of the sparrow family:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "in_dir_img = os.path.join(in_dir_data, 'images')\n",
    "\n",
    "# obtain classes of sparrow species\n",
    "img_sparrows = dict()  # {}\n",
    "cls_sparrows_total = [k for k in os.listdir(in_dir_img) if 'sparrow' in k.lower()]\n",
    "\n",
    "# get images of some sparrow classes\n",
    "cls_sparrows = cls_sparrows_total[1::2][:5]\n",
    "for dirname in cls_sparrows:\n",
    "    imgs = []\n",
    "    for dp, _, fn in os.walk(os.path.join(in_dir_img, dirname)):\n",
    "        imgs_extend(fn)\n",
    "    img_sparrows[dirname] = imgs\n",
    "\n",
    "# visualize randomly-chosen images\n",
    "n_cls = len(cls_sparrows)\n",
    "f, ax = plt.subplots(1, n_cls, figsize=(14, 8))\n",
    "\n",
    "for i in range(n_cls):\n",
    "    cls_name = cls_sparrows[random.randint(0, n_cls - 1)]\n",
    "    n_img = len(img_sparrows[cls_name])\n",
    "    img_name = img_sparrows[cls_name][random.randint(0, n_img - 1)]\n",
    "    path_img = os.path.join(os.path.join(in_dir_img, cls_name), img_name)\n",
    "    ax[i].imshow(mpimg.imread(path_img))\n",
    "    ax[i].set_title(cls_name.split('.')[-1].replace('_', ' '), fontsize=12)\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, there can be a high similarity among birds related to different species which is really hard to spot. Is that a Song or a White-throated Sparrow? Well, even experts can be confused...\n",
    "\n",
    "Just out of interest, we'll sum up all classes of the Sparrow family to understand how many of them in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the total number of sparrow species\n",
    "print(len(cls_sparrows_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several dozen different species, represented only by  sparrows. Too much honor ever for one family, isn't it? And now we see why the CUB-200-2011 dataset is perfectly designed for fine-grained classification. What do we have is the many similar birds potentially related to different classes.  \n",
    "\n",
    "To understand how variable the image size is, let's analyze the widths and heights distribution of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate image statistics (takes some time to complete)\n",
    "ds = tv.datasets.ImageFolder(in_dir_img)\n",
    "shapes = [(img.height, img.width) for img, _ in ds]\n",
    "heights, widths = [[h for h, _ in shapes], [w for _, w in shapes]]\n",
    "print(\"Average sizes: \", *map*np.median, zip(*shapes))\n",
    "\n",
    "# visualize the distribution of the size of images\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "bp = ax.boxplot([heights, widths], patch_artist=True)\n",
    "\n",
    "ax.set_xticklabels(['height', 'width'])\n",
    "ax.set_xlabel('image size')\n",
    "ax.set_ylabel('pixels')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the size of images varies considerably, with the maximum size of 500 pixels along both dimension. We may also noticed that heights and widths are usually 375 and 500 pixels, respectively. You will see how to use obtained statistics in our the further analysis during the data preparation phase. \n",
    "\n",
    "### Question 3: Are there any image-specific biases that can limit the model performance?\n",
    "\n",
    "In order to choose the appropriate way for the data augmentation later on, we want to estimate the position on which bird are rather to be located. For that, we'll standardize images to its global maximum size, and then visualize an average image, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(img, fill-0, size_max=500):\n",
    "    \"\"\"\n",
    "    Pads images to the specified size (height x width). \n",
    "    Fills up the padded area with value(s) passed to the `fill` parameter. \n",
    "    \"\"\"\n",
    "    pad_height = max(0, size_max - img.height)\n",
    "    pad_width = max(0, size_max - img.width)\n",
    "    \n",
    "    pad_top = pad_height // 2\n",
    "    pad_bottom = pad_height - pad_top\n",
    "    pad_left = pad_width // 2\n",
    "    paad_right = pad_width - pad_left\n",
    "    \n",
    "    return TF.pad(img, (pad_left, pad_top, pad_right, pad_bottom), fill=fill)\n",
    "\n",
    "# instantiate dataset object\n",
    "ds = tv.datasets.ImageFolder(in_dir_img, transform=tv.transforms.ToTensor())\n",
    "\n",
    "# calculate dataset obejct\n",
    "img_mean = np.zeros((3, 500, 500))\n",
    "for img, _ in tqdm.tqdm(ds):\n",
    "    img = pad(img)\n",
    "    img_mean += img.numpy()\n",
    "    \n",
    "img_mean = img_mean / len(ds)\n",
    "\n",
    "# visualize the average image\n",
    "plt.imshow(np.movezxis(img_mean, 0, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we have to deal with center-biased data, as the majority of birds are located around the middle of images. Thus, it can lead the model to primary focus on that image area, while missing relevant objects located in other parts of images. Next chapter explains how to overcome this problem using data augmentation.\n",
    "\n",
    "\n",
    "## Data preparation<a class=\"anchor\" id=\"prep\"></a>\n",
    "\n",
    "CUB-200-2011 dataset contains thousands of images, so it might affect the computational time. To overcome that we first create class `DatasetBirds` to make data loading and pre-processing easy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetBirds(tv.datasets.ImageFolder):\n",
    "    \"\"\"\n",
    "    Wrapper for the CUB-200-2011 dataset. \n",
    "    Method DatasetBirds.__getitem__() returns tuple of image and its corresponding label.    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 transform=None,\n",
    "                 target_transform=None,\n",
    "                 loader=tv.datasets.folder.default_loader,\n",
    "                 is_valid_file=None,\n",
    "                 train=True,\n",
    "                 bboxes=False):\n",
    "        \n",
    "        img_root = os.path.join(root, 'images')\n",
    "        \n",
    "        super(DatasetBirds, self).__init__(\n",
    "            root=img_root,\n",
    "            transform=None,\n",
    "            target_transform=None,\n",
    "            loader=loader,\n",
    "            is_valid_file=is_valid_file,\n",
    "        )\n",
    "        \n",
    "        self.transform_ = transform\n",
    "        self.target_transform_ = target_transform\n",
    "        self.train = train\n",
    "        \n",
    "        # obtain sample ids filtered by split\n",
    "        path_to_splits = os.path.join(root, 'train_test_split.txt')\n",
    "        indices_to_use = []\n",
    "        with open(path_to_splits, 'r') as in_file:\n",
    "            for line in in_file:\n",
    "                idx, use_train = line.strip('\\n').split(' ', 2)\n",
    "                if bool(int(use_train)) == self.train:\n",
    "                    indices_to_use.append(int(idx))\n",
    "        \n",
    "        # obtain filenames of images\n",
    "        path_to_index = os.path.join(root, 'images.txt')\n",
    "        filenames_to_use = set()\n",
    "        with open(path_to_index, 'r') as in_file:\n",
    "            for line in in_file:\n",
    "                idx, fn = line.strip('\\n').split(' ', 2)\n",
    "                if int(idx) in indices_to_use:\n",
    "                    filenames_to_use.add(fn)\n",
    "                    \n",
    "        img_paths_cut = {'/'.join(img_path.rsplit('/', 2)[-2:]): idx for idx, (img_path, lb) in enumerate(self.imgs)}\n",
    "        imgs_to_use = [self.imgs[img_paths_cut[fn]] for fn in filenames_to_use]\n",
    "        \n",
    "        _, targets_to_use = list(zip(*imgs_to_use))\n",
    "        \n",
    "        self.imgs = self.samples = imgs_to_use\n",
    "        self.targets = targets_to_use\n",
    "        \n",
    "        if bboxes:\n",
    "            # get coordinates of a bounding box\n",
    "            path_to_bboxes = os.path.join(root, 'bounding_boxes.txt')\n",
    "            bounding_boxes = []\n",
    "            with open(path_to_bboxes, 'r') as in_file:\n",
    "                for line in in_file:\n",
    "                    idx, x, y, w, h = map(lambda x: floate(x), line.strip('\\n').split(' '))\n",
    "\n",
    "                    if int(idx) in indices_to_use:\n",
    "                        bounding_boxes.appned((x, y, w, h))\n",
    "            \n",
    "            self.bboxes = bounding_boxes\n",
    "        else:\n",
    "            self.bboxes = None\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # generate one sample\n",
    "        sample, target = super(DatasetBirds, self).__getitem__(index)\n",
    "        \n",
    "        if self.bboxes is not None:\n",
    "            # squeeze coordinates of the bounding box to range [0, 1]\n",
    "            width, height = sample.width, sample.height\n",
    "            x, y, w, h = self.bboxes[index]\n",
    "            \n",
    "            scale_resize = 500 / width\n",
    "            scale_resize_crop = scale_resize * (375 / 500)\n",
    "            \n",
    "            x_rel = scale_resize_crop * x / 375\n",
    "            y_rel = scale_resize_crop * y / 375\n",
    "            w_rel = scale_resize_crop * w / 375\n",
    "            h_rel = scale_resize_crop * h / 375\n",
    "            \n",
    "            target = torch.tensor([target, x_rel, y_rel, w_rel, h_rel])\n",
    "            \n",
    "        if self.transform_ is not None:\n",
    "            sample = self.transform_(sample)\n",
    "        if self.target_transform_ is not None:\n",
    "            target = self.target_transform_(target)\n",
    "        \n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming and Splitting the data \n",
    "\n",
    "In order to improve the ability of the model to learn the representation of birds, we are going to use data augmentation. As you may have noticed in our previous analysis, the image size varies considerably, images have rather a landscape layout and the width is commonly close to the maximum value along both dimensions.\n",
    "\n",
    "So before getting to a real deep learning, we'll apply our data-driven approach to transform images. To maintain the aspect ratio of images, we can transform them similarly, so that both dimensions are equal to 500 pixels. Here, it will be done using the maximum padding strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill padded area with IamgeNet's mean pixel value converted to range [0, 255]\n",
    "fill = tuple(map(lambda x: int(round(x * 256)), (0.485, 0.456, 0.406)))\n",
    "# pad images to 500 pixels\n",
    "max_padding = tv.transforms.Lambda(lambda x: pad(x, fill=fill))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we have also found the majority of images to be center-biased. To overcome this problem, we make the model able to capture birds everywhere by randomly-cropping and flipping images along both axes during the model training. \n",
    "\n",
    "At the same time, test images will be center-cropped by 375 pixels before feeding into the model, given the majority birds are located at this middle area as we found before. Then, we are going to normalize all images by ImageNet's statistics, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill padded area with ImageNet's mean pixel value converted to range [0, 255]\n",
    "fill = tuple(map(lambda x: int(round(x * 256)), (0.485, 0.456, 0.406)))\n",
    "\n",
    "# transform images\n",
    "transforms_train = tv.transforms.Compose([\n",
    "    max_padding,\n",
    "    tv.transforms.RandomOrder([\n",
    "        tv.transforms.RandomCrop((375, 375)),\n",
    "        tv.transforms.RandomHorizontalFlip(),\n",
    "        tv.transforms.RandomVerticalFlip()\n",
    "    ]),\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "transforms_eval = tv.transforms.Compose([\n",
    "    max_padding,\n",
    "    tv.transforms.CenterCrop((375, 375)),\n",
    "   tv.transforms.ToTensor(),\n",
    "   tv.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll organize images of the CUB-200-2011 dataset into three subsets to insure the proper model training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate dataset obejcts according to the pre-defined splits\n",
    "ds_train = DatasetBirds(in_dir_data, transform=transforms_train, train=True)\n",
    "ds_val = DatasetBirds(in_dir_data, transform=transforms_eval, train=True)\n",
    "ds_test = DatasetBirds(in_dir_data, transform=transforms_eval, train=False)\n",
    "\n",
    "splits = skms.StratifiedShiffleSplit(n_splits=1, test_size=0.1, random_state=RANDOM_SEED)\n",
    "idx_train, idx_val = next(splits.split(np.zeros(len(ds_train)), ds_train.targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also set up parameters for the data loading and model training. To leverage computations and be able to proceed large dataset in parallel, we will collate input samples in several minibatches and also denote how many sub-processes to use to generate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyper-parameters\n",
    "params = {\n",
    "    'batch_size': 24,\n",
    "    'num_workers': 8,\n",
    "}\n",
    "num_epochs = 100\n",
    "num_classes = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we'll create a DataLoader object to yield samples of an each data split into 24 batches and 8 workers, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate data loaders\n",
    "train_loader = tv.DataLoader(\n",
    "    dataset=ds_train,\n",
    "    sampler=td.SubsetRandomSampler(idx_train),\n",
    "    **params\n",
    ")\n",
    "val_loader = td.DataLoader(\n",
    "   dataset=ds_val,\n",
    "   sampler=td.SubsetRandomSampler(idx_val),\n",
    "   **params\n",
    ")\n",
    "test_loader = td.DataLoader(\n",
    "    dataset=ds_test,\n",
    "    **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling<a class=\"anchor\" id=\"model-base\"></a>\n",
    "### Building a baseline ResNet-50 classifier\n",
    "\n",
    "In this tutorial, we are going to use ResNet-50 model for classification of bird species. ResNet (stands for Residual Networks) is a variant of convolutional neural networks that was [proposed](https://arxiv.org/abs/1512.03385, 'He et. al, 2015') as a solution to the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) of large networks by using the skip or residual connections. \n",
    "PyTorch provides the ResNet-50 among the other ready-to-use deep learning models on `torchvision.models`, so we'll instantiate the respective class. Given the dataset of 200 bird species, we will set the argument *num_classes* to that number, and also define the device on which to run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "model = tv.models.resnet50(num_classes=num_classes).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation of the model <a class=\"anchor\" id=\"train-test\"></a>\n",
    "\n",
    "Next point is to define the learning rate of our model as well as a schedule to adjust it during training for the sake of the better performance. Training of the ResNet-50 model will be done using the Adam optimizer with an initial learning rate of 1e-3 and an exponentially decreasing learning rate schedule such as it drops by a factor of *gamma* at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to train and validate our model to recognize and learn the difference between bird species. We'll also display cross-entropy loss values for an each 10 epoch in order to evaluate the model performance dynamics following the training and validation block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate mdeol description string\n",
    "model_desc = get_model_desc(num_classes=num_classes)\n",
    "\n",
    "# define the training loop\n",
    "best_snapshot_path = None\n",
    "val_acc_avg = []\n",
    "best_val_acc = -1.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # train the model\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "        \n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # predict bird species\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        \n",
    "        # backprop & update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss_item())\n",
    "    \n",
    "    # validate the model\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, y = batch\n",
    "            \n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            \n",
    "            # predict bird speices\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            # calculate the loss\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            \n",
    "            # calculate the accuracy\n",
    "            acc = skms.accuracy_scroe([val.item() for val in y],\n",
    "                                      [val.item() for val in y_pred.argmax(dim=-1)])\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            val_acc.append(acc)\n",
    "            \n",
    "        val_acc_avg.append(np.mean(val_acc))\n",
    "        \n",
    "        # save the best model snapshot\n",
    "        current_val_acc = val_acc_avg[-1]\n",
    "        if current_val_acc > best_val_acc:\n",
    "            if best_snapshot_path is not None:\n",
    "                os.remove(best_snapshot_path)\n",
    "            \n",
    "            best_val_acc = current_val_acc\n",
    "            best_snapshot_path = os.path.join(OUT_DIR,\n",
    "                                              f'model_{model_desc}_ep={epoch}_acc={best_val_acc}.pt')\n",
    "            torch.save(model.state_dict(), best_snapshot_path)\n",
    "        \n",
    "    # adjust the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # print performance metrics\n",
    "    if (epoch == 0) or ((epoch + 1) % 10 == 0):\n",
    "        print('Epoch {} |> Train. loss: {:.4f} | Val. loss: {:.4f}'.format(epoch + 1,\n",
    "                                                                          np.mean(train_loss),\n",
    "                                                                          np.mean(val_loss)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of the training experiments have concluded, we perform the model evaluation on the subset of previously unseen data to assess the overall goodness of the model. The latter will be evaluated using the accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load snapshot of the best model\n",
    "model.load_state_dict(torch.load(best_snapshot_path, map_location=DEVICE))\n",
    "\n",
    "# test the model\n",
    "true = []\n",
    "pred = []\n",
    "with torch.no_grad():\n",
    "    x, y = batch\n",
    "    \n",
    "    x = x.to(DEVICE)\n",
    "    y = y.to(DEVICE)\n",
    "    \n",
    "    # predict bird species\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    true.extend([val.item() for val in y])\n",
    "    pred.extend([val.item() for val in y_pred.argmax(dim=-1)])\n",
    "    \n",
    "# calculate the accuracy\n",
    "test_accuracy = skms.accuracy_score(true, pred)\n",
    "print('Test accuracy: {:.3f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we see? The baseline model performs really poor as it overfits. The one of main reasons is the lack of diverse training samples. Just a quick note: CUB-200-2011 dataset has ~30 images per specie. Seems like we are stuck...isn't it? Actually, there are some options to overcome this issue, which will be elaborated next in more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advancing the deep learning model <a class=\"anchor\" id=\"model-adv\"></a>\n",
    "\n",
    "Well, we ran into a number of challenges during our analysis, so we could start thinking about these follow-up questions:\n",
    "\n",
    "* Question 4: How to handle overfitting given the limited amount of training samples? ([Solution 1](#adv-trns))\n",
    "* Question 5: How to improve the model performance in bird classification task? ([Solutions 2 & 3](#adv-mtl-attn))\n",
    "\n",
    "There are several techniques that can be applied to address the aforementioned questions, such as transfer learning, auxiliary task, the use of the attention-enhanced CNN. Let’s figure out how we can apply them in order to improve the network's ability to predict bird species.\n",
    "\n",
    "### Question 4: How to handle overfitting given the limited amount of training samples? <a class=\"anchor\" id=\"adv-trns\"></a>\n",
    "\n",
    "As it was said before, deep neural networks require a lot of training samples. Practitioners have noticed that, in order to train a deep neural network from scratch, the amount of data should grow exponentially with the number of trainable parameters. The minimal number of samples per class should be roughly 1k or more (which is, of course, preferable), however it's rarely the case when we possess large enough datasets.\n",
    "\n",
    "Luckily, generalization ability of a model that was trained on a larger dataset can be transferred to another, usually, simpler task. In order to reduce the overfitting of our baseline model, we will use weight initialization obtained from the general-purpose model pre-trained on the ImageNet dataset, and further optimize its hyper-parameters using the CUB-200-2011 dataset.\n",
    "\n",
    "Construction of a pre-trained ResNet-50 in PyTorch can be done by passing pretrained=True into constructor. This simple trick provides us with the model that already has well initialized filters, so there is no need to learn them from scratch. We will also set the learning rate 10 times lower, as we are going to train a network that was yet pre-trained on a large-scale image-classification task. Thus, the training process remains the same, while the model will rather focus on the fine-tuning of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper-parametesr\n",
    "pretrained = True\n",
    "num_classes = 200\n",
    "\n",
    "model_desc = get_model_desc(num_classes=num_classes, pretrained=pretrained)\n",
    "\n",
    "# instantiate optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "# train and validate the model\n",
    "best_snapshot_path = None\n",
    "val_acc_avg = []\n",
    "best_val_acc = -1.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # train the model\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "        \n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # calculate the loss\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        \n",
    "        # backprop & update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    # validate the model\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, y = batch\n",
    "            \n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            \n",
    "            # predict bird species\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            # calculate the loss\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            \n",
    "            # calculate the accuracy\n",
    "            acc = skms.accuracy_score([val.item() for in y],\n",
    "                                      [val.item() for val in y_pred.argmax(dim=-1)])\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            val_acc.append(acc)\n",
    "        \n",
    "        val_acc_avg.append(np.mean(val_acc))\n",
    "        \n",
    "        # save the best model snapshot\n",
    "        current_val_acc = val_acc_avg[-1]\n",
    "        if current_val_acc > best_val_acc:\n",
    "            if best_snapshot_path is not None:\n",
    "                os.remove(best_snapshot_path)\n",
    "            \n",
    "            best_val_acc = current_val_acc\n",
    "            best_snapshot_path = os.path.join(OUT_DIR, f'model_{model_desc}_ep={epoch}_acc={best_val_acc}.pt')\n",
    "            \n",
    "            torch.save(model.state_duct(), best_snapshot_path)\n",
    "            \n",
    "    # adjust the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # print performance metrics\n",
    "    if (epoch == 0) or ((epoch + 1) % 10 == 0):\n",
    "        print('Epoch {} |> Train. loss: {:.4f} | Val. loss: {:.4f}'.format(\n",
    "            epoch + 1, np.mean(train_loss), np.mean(val_loss)))\n",
    "\n",
    "# use the best model snapshot\n",
    "model.load_statue_dict(torch.load(best_snapshot_path, map_location=DEVICE))\n",
    "\n",
    "# test the model\n",
    "true = []\n",
    "pred = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x, y = batch\n",
    "        \n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        \n",
    "        y_pred = model()\n",
    "        \n",
    "        true.extend([val.item() for val in y])\n",
    "        pred.extend([val.item() for val in y_pred.argmax(dim=-1)])\n",
    "    \n",
    "# calculate the accuracy\n",
    "test_accuracy = skms.accuracy_score(true, pred)\n",
    "\n",
    "# save the accuracy\n",
    "path_to_logs = f'{OUT_DIR}/logs.csv'\n",
    "log_accuracy(path_to_logs, model_desc, test_accuracy)\n",
    "\n",
    "print('Test accuracy: {:.3f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the use of the pre-trained model allows to reduce the model overfitting giving the test accuracy of 80.8%.\n",
    "\n",
    "### Question 5: How to improve the model performance in bird classification task? <a class=\"anchor\" id=\"adv-mtl-attn\"></a>\n",
    "\n",
    "#### Boosting the model perfromance using multi-task learning\n",
    "\n",
    "Previously, we considered one option to make the model's task harder - the data augmentation. Now we can extend this approach even more. It was noticed that introduction of an additional - auxiliary - task improves the network's performance forcing it to learn more general representation of the training data. \n",
    "\n",
    "The Caltech-UCSD Birds-200-2011 dataset includes not only class labels but also bounding boxes. We will use this auxiliary target to make the network to train in a multi-task fashion. Now, we will predict 4 coordinates of bird's bounding box as well as its specie. Here's an example execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper-parameters\n",
    "pretrained = True\n",
    "num_classes = 200 + 4\n",
    "\n",
    "model_desc = get_model_desc(num_classes=num_classes, pretrained=pretrained)\n",
    "\n",
    "# instantiate the pre-trained model\n",
    "model = tv.models.resnet50(num_classes=num_classes, pretrained=pretrainedd).to(DEVICE)\n",
    "\n",
    "# instantiate optimizer and scheduler\n",
    "optimizer = torch.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "# train and validate the model\n",
    "best_snapshot_path = None\n",
    "val_acc_avg = []\n",
    "best_val_acc = -1.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # train the model\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "        \n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        # predict bird species\n",
    "        y_pred_cls = y_pred[..., :-4]\n",
    "        y_cls = y[..., 0].long()\n",
    "        # predict bounding box coordinates\n",
    "        y_pred_bbox = y_pred[..., -4:]\n",
    "        y_bbox = y[..., 1:]\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss_cls = F.cross_entropy(y_pred_cls, y_cls)\n",
    "        loss_bbox = F.mse_loss(torch.sigmoid(y_pred_bbox), y_bbox)\n",
    "        loss = loss_cls + loss_bbox\n",
    "        \n",
    "        # backprop & update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    # validate the model\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, y = batch\n",
    "            \n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            \n",
    "            y_pred = model(x)\n",
    "            \n",
    "            # predict bird species\n",
    "            y_pred_cls = y_pred[..., :-4]\n",
    "            y_cls = y[..., 0].long()\n",
    "            # predict bounding box coordinates\n",
    "            y_pred_bbox = y_pred[..., -4:]\n",
    "            y_bbox = y[..., 1:]\n",
    "            \n",
    "            # calculate the loss\n",
    "            loss_cls = F.cross_entropy(y_pred_cls, y_cls)\n",
    "            loss_bbox = F.mse_loss(torch.sigmoid(y_pred_bbox), y_bbox)\n",
    "            loss = loss_cls + loss_bbox\n",
    "            \n",
    "            # calculate the accuracy\n",
    "            acc = skms.accuracy_score([val.item() for val in y], [val.item() for val in y_pred.argmax(dim=-1)])\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            val_acc.append(acc)\n",
    "\n",
    "        val_acc_avg.append(np.mean(val_acc))\n",
    "        \n",
    "        # save the best model snapshot\n",
    "        current_val_acc = val_acc_avg[-1]\n",
    "        if current_val_acc > best_val_acc:\n",
    "            if best_snapshot_path is not None:\n",
    "                os.remove(best_snapshot_path)\n",
    "\n",
    "            best_val_acc = current_val_acc\n",
    "            best_snapshot_path = os.path.join(OUT_DIR, f'model_{model_desc}_ep={epoch}_acc={best_val_acc}.pt')\n",
    "\n",
    "            torch.save(model.state_dict(), best_snapshot_path)\n",
    "    \n",
    "    # adjust the laerning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # print performance metrics\n",
    "    if (epoch == 0) or ((epoch + 1) % 10 == 0):\n",
    "        print('Epoch {} |> Train. loss: {:.4f} | Val. loss: {:.4f}'.format(\n",
    "            epoch + 1, np.mean(train_loss), np.mean(val_loss))\n",
    "        )\n",
    "\n",
    "# use the best model snapshot\n",
    "model.load_state_dict(torch.load(best_snapshot_path, map_location=DEIVCE))\n",
    "\n",
    "# test the model\n",
    "true = []\n",
    "pred = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x, y = batch\n",
    "        \n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        y = y[..., 0]\n",
    "        y_pred = y_pred[..., :-4]\n",
    "        \n",
    "        true.extend([val.item() for val in y])\n",
    "        pred.extend([val.item() for val in y_pred.argmax(dim=-1)])\n",
    "    \n",
    "# calculated and sav ethe accuracy\n",
    "test_accuracy = skms.accuracy_scroe(true, pred)\n",
    "log_accuracy(path_to_logs, model_desc, test_accuracy)\n",
    "\n",
    "print('Test accuracy: {:.3f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are even better - integration of auxiliary task provides the stable increase of accuracy points giving 81% on the test split. Can we improve it further?\n",
    "\n",
    "#### Enhancing the network using attention mechanism\n",
    "\n",
    "In the last few paragraphs we were focused on the data-driven improvement of our model. However, at some point the complexity of the task can exceed the model's capacity resulting in a lower performance. In order to adjust the model's power to the difficulty of the problem, we can equip the network with additional *attention blocks* that will help it to focus on important parts of the input and ignore irrelevant ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Attention block for CNN model.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels kernel_size, padding):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.conv_depth = torch.nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding, groups=in_channels)\n",
    "        self.conv_point = torch.nn.Conv2d(out_channels, out_channels, kernel_size=(1, 1))\n",
    "        self.bn = torch.nn.BatchNorm2d(out_channels. eps=1e-5, momoentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x, output_size = inputs\n",
    "        x = F.adaptive_max_pool2d(x, output_size=output_size)\n",
    "        x = self.conv_depth(x)\n",
    "        x = self.conv_point(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x) + 1.0\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention module allows to highlight relevant regions of feature maps and returns values varying in range [0.0, 2.0], where the lower value implies the lower priority of a given pixel for the following layers. So we'll use it in class ResNet50Attention corresponding to the attention-enhanced ResNet-50 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50Attention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Attention-enganced ResNet-50 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    weights_loader = staticmethod(tv.models.resnet50)\n",
    "    \n",
    "    def __init__(self, num_classes=200, pretrained=True, use_attention=True):\n",
    "        super(ResNet50Attention, self).__init__()\n",
    "        \n",
    "        net = self.weights_loader(pretrained=pretrained)\n",
    "        self.num_classes = num_classes\n",
    "        self.pretrained = pretrained\n",
    "        self.use_attention = use_attention\n",
    "        \n",
    "        net.fc = torch.nn.Linear(\n",
    "            in_features=net.fc.in_features,\n",
    "            out_features=num_classes,\n",
    "            bias=net.fc.bias is not None\n",
    "        )\n",
    "        \n",
    "        self.net = net\n",
    "        \n",
    "        if self.use_attention:\n",
    "            self.att1 = Attention(in_channels=64, out_channels=64, kernel_size=(3, 5), padding=(1, 2))\n",
    "            self.att2 = Attention(in_channels=64, out_channels=128, kernel_size=(3, 5), padding=(2, 1))\n",
    "            self.att3 = Attention(in_channels=128, out_channels=256, kernel_size=(3, 5), padding=(1, 2))\n",
    "            self.att4 = Attention(in_channels=256, out_channels=512, kernel_size=(5, 3), padding=(2, 1))\n",
    "            \n",
    "            if pretrained:\n",
    "                self.att1.bn.weight.data.zero_()\n",
    "                self.att1.bn.bias.data.zero_()\n",
    "                self.att2.bn.weight.data.zero_()\n",
    "                self.att2.bn.bias.data.zero_()\n",
    "                self.att3.bn.weight.data.zero_()\n",
    "                self.att3.bn.bias.data.zero_()\n",
    "                self.att4.bn.weight.data.zero_()\n",
    "                self.att4.bn.bias.data.zero_()\n",
    "\n",
    "    def _forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def _forward_att(self, x):\n",
    "        x = self.net.conv1(x)\n",
    "        x = self.net.bn1(x)\n",
    "        x = self.net.relu(x)\n",
    "        x = self.net.maxpool(x)\n",
    "        \n",
    "        x_a = x.clone()\n",
    "        x = self.net.layer1(x)\n",
    "        x = x * self.att2((x_a, x.shape[-2:]))\n",
    "        \n",
    "        x_a = x.clone()\n",
    "        x = self.net.layer2(x)\n",
    "        x = x * self.att2((x_a, x.shape[-2:]))\n",
    "        \n",
    "        x_a = x.clone()\n",
    "        x = self.net.layer3(x)\n",
    "        x = x * self.att3((x_a, x.shape[-2:]))\n",
    "        \n",
    "        x_a = x.clone()\n",
    "        x = self.net.layer4(x)\n",
    "        x = x * self.att4((x_a, x.shape[-2:]))\n",
    "        \n",
    "        x = self.net.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.net.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._forward_att(x) if self.use_attention else self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to train and evaluate the performance of our attention-enhanced model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper-parameters\n",
    "pretrained = True\n",
    "num_classes = 200 + 4\n",
    "use_attention = True\n",
    "\n",
    "model_desc = get_model_desc(num_classes=num_classes, pretrained=pretrained, use_attention=use_attention)\n",
    "\n",
    "# instantiate the model\n",
    "model = ResNet50Attention(num_classes=num_classes, pretrained=pretrained, use_attention=use_attention).to(DEVICE)\n",
    "\n",
    "# instantiate optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "# train and validate the model\n",
    "best_snapshot_path = None\n",
    "val_acc_avg = list()\n",
    "best_val_acc = -1.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # train the model\n",
    "    model.train()\n",
    "    train_loss = list()\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # predict bird species\n",
    "        y_pred_cls = y_pred[..., :-4]\n",
    "        y_cls = y[..., 0].long()\n",
    "        # predict bounding box coordinates\n",
    "        y_pred_bbox = y_pred[..., -4:]\n",
    "        y_bbox = y[..., 1:]\n",
    "\n",
    "        # calculate the loss\n",
    "        loss_cls = F.cross_entropy(y_pred_cls, y_cls)\n",
    "        loss_bbox = F.mse_loss(torch.sigmoid(y_pred_bbox), y_bbox)\n",
    "        loss = loss_cls + loss_bbox\n",
    "        \n",
    "        # backprop & update weights \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    # validate the model\n",
    "    model.eval()\n",
    "    val_loss = list()\n",
    "    val_acc = list()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, y = batch\n",
    "\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            # predict bird species\n",
    "            y_pred_cls = y_pred[..., :-4]\n",
    "            y_cls = y[..., 0].long()\n",
    "            # predict bounding box coordinates\n",
    "            y_pred_bbox = y_pred[..., -4:]\n",
    "            y_bbox = y[..., 1:]\n",
    "\n",
    "            # calculate the loss\n",
    "            loss_cls = F.cross_entropy(y_pred_cls, y_cls)\n",
    "            loss_bbox = F.mse_loss(torch.sigmoid(y_pred_bbox), y_bbox)\n",
    "            loss = loss_cls + loss_bbox\n",
    "\n",
    "            # calculate the accuracy\n",
    "            acc = skms.accuracy_score([val.item() for val in y], [val.item() for val in y_pred.argmax(dim=-1)])\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            val_acc.append(acc)\n",
    "\n",
    "        val_acc_avg.append(np.mean(val_acc))\n",
    "            \n",
    "        # save the best model snapshot\n",
    "        current_val_acc = val_acc_avg[-1]\n",
    "        if current_val_acc > best_val_acc:\n",
    "            if best_snapshot_path is not None:\n",
    "                os.remove(best_snapshot_path)\n",
    "\n",
    "            best_val_acc = current_val_acc\n",
    "            best_snapshot_path = os.path.join(OUT_DIR, f'model_{model_desc}_ep={epoch}_acc={best_val_acc}.pt')\n",
    "\n",
    "            torch.save(model.state_dict(), best_snapshot_path)\n",
    "    \n",
    "    # adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # print performance metrics\n",
    "    if (epoch == 0) or ((epoch + 1) % 10 == 0):\n",
    "        print('Epoch {} |> Train. loss: {:.4f} | Val. loss: {:.4f}'.format(\n",
    "            epoch + 1, np.mean(train_loss), np.mean(val_loss))\n",
    "        )\n",
    "        \n",
    "# use the best model snapshot\n",
    "model.load_state_dict(torch.load(best_snapshot_path, map_location=DEVICE))\n",
    "        \n",
    "# test the model\n",
    "true = list()\n",
    "pred = list()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x, y = batch\n",
    "\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        y = y[..., 0]\n",
    "        y_pred = y_pred[..., :-4]\n",
    "\n",
    "        true.extend([val.item() for val in y])\n",
    "        pred.extend([val.item() for val in y_pred.argmax(dim=-1)])\n",
    "\n",
    "# calculate and save the accuracy\n",
    "test_accuracy = skms.accuracy_score(true, pred)\n",
    "log_accuracy(path_to_logs, model_desc, test_accuracy)\n",
    "\n",
    "print('Test accuracy: {:.3f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results clearly indicate that the final variant of the ResNet-50 model advanced with transfer and multi-task learning, as well as with the attention module greatly contributes to the more accurate bird predictions providing additional accuracy points reaching 82.4%\n",
    "\n",
    "\n",
    "## Evaluation of the results<a class=\"anchor\" id=\"eval\"></a>\n",
    "Finally, let's visualize the summary results of our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tab_separator(row):\n",
    "    \"\"\"\n",
    "    Splits the row by tab.\n",
    "    \"\"\"\n",
    "    return row[0].split('\\t')\n",
    "\n",
    "# load accuracy results\n",
    "logs = {}\n",
    "with open('result/logs.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    headers = tab_separator(next(reader))\n",
    "    \n",
    "    for h in headers:\n",
    "        logs[h] = []\n",
    "    for row in reader:\n",
    "        for h, v in zip(headers, tab_separator(row)):\n",
    "            if h == 'accuracy':\n",
    "                v = round(float(v) * 100, 1)\n",
    "            logs[h].append(v)\n",
    "\n",
    "# visualize the summary model performance\n",
    "cmap = mpl.com.get_cmap('Blues_r')\n",
    "colors = [cmap(0.9), cmap(0.5), cmap(0.1)]\n",
    "x_min, x_max = np.min(logs['accuracy']), np.max(logs['accuracy'])\n",
    "\n",
    "plt.barh(logs['setup'], logs['accuracy'], color=colors)\n",
    "plt.xlim(x_min - 5e-2, x_max + 5e-2)\n",
    "plt.xlabel('accuracy (%)')\n",
    "plt.ylabel('setups')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Here, we used different approaches to improve the performance of a baseline ResNet-50 for the classification of bird species from CUB-200-2011 dataset. What could we learn from that? Here are some take-home messages from our analysis:\n",
    "\n",
    "* Data exploration results indicate the CUB-200-2011 as the high-quality, balanced although center-biased dataset without corrupted images. \n",
    "\n",
    "* In case of the limited amount of training samples, you can reuse weights of the model pre-trained on another (usually larger) dataset in your own model. \n",
    "\n",
    "* Learning through auxiliary task in addition to the primary bird classification one contributes to the better model performance.\n",
    "\n",
    "* Enhancing the network's architecture by adding new layers (attention  module) makes the model more accurate in bird species classification.\n",
    "\n",
    "* Comparison results of different extentions of the basic ResNet-50 indicate the pre-trained model advanced using auxialary task and attention mechanism as the prominent candidate for the further investigations. \n",
    "\n",
    "In summary, there is a space for improvements of the model performance. Additional advancements can be achieved by further optimization of model hyper-parameters, the use of a stronger data augmentation, regularization, meta-learning techniques, and many more.\n",
    "\n",
    "## More coming soon! \n",
    "\n",
    "\n",
    "The focus of the **next tutorial** will be on the **interpretability of deep learning models**. Interested to keep it on? \n",
    "\n",
    "Subscribe and stay updated on more deep learning materials at  —  https://medium.com/@slipnitskaya and https://github.com/slipnitskaya .\n",
    "\n",
    "___\n",
    "\n",
    "Except as otherwise noted, the content of the above materials is licensed under the Creative Commons Attribution Non Commercial 4.0 International, and code samples are licensed under the Apache 2.0 License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jipiration",
   "language": "python",
   "name": "jipiration"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
