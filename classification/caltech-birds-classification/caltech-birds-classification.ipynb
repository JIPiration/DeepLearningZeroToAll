{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Tutorial: Bird by Bird using Deep Learning</h1>\n",
    "<h2 align=\"center\">Advancing deep learning models for fine-grained classification of bird species</h2>\n",
    "<h3 align=\"center\">Author: Sofya Lipnitskaya</h3>\n",
    "\n",
    "### This repository related to [Bird by Bird using Deep Learning](https://github.com/slipnitskaya/caltech-birds-advanced-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "With this tutorial, you will tackle such an established problem in computer vision as fine-grained classification of bird species. The notebook demonstrates how to classify bird images from the Caltech-UCSD Birds-200-2011 ([CUB-200-2011](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)) dataset using PyTorch, one of the most popular open-source frameworks for deep learning experiments. \n",
    "\n",
    "**Project outline** \n",
    "\n",
    "Here you can get familiarized with the content more properly (the respective CRISP-DM stages of the project are indicated in parentheses):\n",
    "\n",
    "* Introducing the bird species recognition problem [(Business Understanding)](#motiv)\n",
    "* Exploratory analysis of CUB-200-2011 dataset [(Data Understanding)](#data)\n",
    "* Transforming images and splitting the data [(Data Preparation)](#prep) \n",
    "* Training and evaluation of the baseline model [(Modelling. p. 1/2)](#model-base)\n",
    "* Advancing the deep learning model [(Modelling. p. 2/2)](#model-adv) \n",
    "* Conclusions and Future work [(Evaluation)](#eval)\n",
    "\n",
    "**Learning Goals**\n",
    "\n",
    "By the end of the tutorial, you will be able to:\n",
    "- Understand basics of image classification problem of bird species.\n",
    "- Determine the data-driven image pre-processing strategy.\n",
    "- Create your own deep learning pipeline for image classification.\n",
    "- Build, train and evaluate ResNet-50 model to predict bird species.\n",
    "- Improve the model performance by using different techniques.\n",
    "\n",
    "***\n",
    "\n",
    "## Introducing the bird species recognition problem<a class=\"anchor\" id=\"motiv\"></a>\n",
    "\n",
    "**Motivation** \n",
    "\n",
    "Bird species recognition is a difficult task challenging the visual abilities for both human experts and computers. One of the interesting task related to that problem implies the classification of birds by species using imagery data collected from aerial surveys. Bird populations are important biodiversity indicators, so collecting reliable data is quite [important](https://dl.acm.org/doi/10.1016/j.patrec.2015.08.015) to ecologists. Recognition of bird species also benefits companies developing wind farms producing renewable energy, since their construction requires the prior risk assessment of bird collisions, threatening many of the world’s species with extinction.\n",
    "\n",
    "This, of course, would be a very ambitious plan to try to find the solution for this problem within a single notebook, so let's make it simple and focus on the bird classification. To make it even more concise, here, we are going to create and evaluate a deep learning model to classify bird images from the Caltech-UCSD Birds-200-2011 ([CUB-200-2011](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)) dataset. In this tutorial, you will learn how to perform the data-driven image pre-processing, build a baseline ResNet-based classifier, and to improve its performance for even better results in bird species recognition using different techniques, which will be described later on.\n",
    "\n",
    "**Questions to be solved through the notebook:**\n",
    "\n",
    "1. Do corrupted images exist in our dataset?\n",
    "2. What would be the optimal data transformation strategy?\n",
    "3. Are there any image-specific biases that can limit the model performance?\n",
    "4. How to handle overfitting given the limited amount of training samples?\n",
    "5. How to improve the model performance in bird species recognition?\n",
    "\n",
    "***\n",
    "\n",
    "First, let's import packages that we will use in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "#import targfile\n",
    "import multiprocessing as mp\n",
    "\n",
    "import tqdm\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.model_selection as skms\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as td\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision as tv\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# define constants\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "OUT_DIR = 'results'\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# create an output  folder\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    try:\n",
    "        os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    except OSError as exc:\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise Exception('OSError')\n",
    "\n",
    "            \n",
    "def get_model_desc(pretrained=False, num_classes=200, use_attention=False):\n",
    "    \"\"\"\n",
    "    Generates description string.\n",
    "    \"\"\"\n",
    "    desc = []\n",
    "    \n",
    "    if pretrained:\n",
    "        desc.append('Transfer')\n",
    "    else:\n",
    "        desc.append('Baseline')\n",
    "        \n",
    "    if num_classes == 204:\n",
    "        desc.append('Multitask')\n",
    "\n",
    "    if use_attention:\n",
    "        desc.append('Attention')\n",
    "    \n",
    "    return '-'.join(desc)\n",
    "\n",
    "\n",
    "def log_accuracy(path_to_csv, desc, acc, sep='\\t', newline='\\n'):\n",
    "    \"\"\"\n",
    "    Logs accuracy into a CSV-file\n",
    "    \"\"\"\n",
    "    file_exists = os.path_exists(path_to_csv)\n",
    "    \n",
    "    mode = 'a'\n",
    "    if not file_exists:\n",
    "        mode += '+'\n",
    "    \n",
    "    with open(path_to_csv, mode) as csv:\n",
    "        if not file_exists:\n",
    "            csv.write(f'setup{sep}accuracy{newline}')\n",
    "        \n",
    "        csv.write(f'{desc}{sep}{acc}{newline}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection<a class=\"anchor\" id=\"data\"></a>\n",
    "\n",
    "In this tutorial, we are going to use CUB-200-2011 dataset consisting of 11788 images of birds belonging to 200 species. \n",
    "\n",
    "The dataset file can be downloaded and extracted manually from [link](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html 'www.vision.caltech.edu'), or, alternatively, using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleDriveDownloader(object):\n",
    "    \"\"\"\n",
    "    Downloading a file stored on Google Drive by its URL.\n",
    "    If the link is pointing to another resource, the redirect chain is being expanded.\n",
    "    \n",
    "    Return the output path.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_url = 'https://docs.google.com/uc?export=download'\n",
    "    chunk_size = 32768\n",
    "    \n",
    "    def __init__(self, url, out_dir):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.out_name = url.rsplit('/', 1)[-1]\n",
    "        self.url = self._get_redirect_url(url)\n",
    "        self.out_dir = out_dir\n",
    "        \n",
    "    @staticmethod\n",
    "    def _get_redirect_url(url):\n",
    "        response = requests.get(url)\n",
    "        if response.url != url and response.url is not None:\n",
    "            redirect_url = response.url\n",
    "            return redirect_url\n",
    "        else:\n",
    "            return url\n",
    "        \n",
    "    @staticmethod\n",
    "    def _get_confirm_token(response):\n",
    "        for key, value in response.cookies.items():\n",
    "            if key.startswith('download_warning'):\n",
    "                return value\n",
    "        return None\n",
    "    \n",
    "    def _save_response_content(self, response):\n",
    "        with open(self.fpath, 'wb') as f:\n",
    "            bar = tqdm.tqdm(total=None)\n",
    "            progress = 0\n",
    "            for chunk in response.iter_content(self.chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    progress += len(chunk)\n",
    "                    bar.update(progress - bar.n)\n",
    "            bar.close()\n",
    "    \n",
    "    @property\n",
    "    def file_id(self):\n",
    "        return self.url.split('?')[0].split('/')[-2]\n",
    "    \n",
    "    @property\n",
    "    def fpath(self):\n",
    "        return os.path.join(self.out_dir, self.out_name)\n",
    "    \n",
    "    def download(self):\n",
    "        os.makedirs(self.out_dir, exist_ok=True)\n",
    "        \n",
    "        if os.path.isfile(self.fpath):\n",
    "            print(\"File is downloaded yet: \", self.fpath)\n",
    "        else:\n",
    "            session = requests.Session()\n",
    "            response = session.get(self.base_url, params={'id': self.file_id}, stream=True)\n",
    "            token = self._get_confirm_token(response)\n",
    "            \n",
    "            if token:\n",
    "                response = session.get(self.base_url, params={'id': self.file_id, 'confirm': token}, stream=True)\n",
    "            else:\n",
    "                raise RuntimeError()\n",
    "                \n",
    "            self._save_response_content(response)\n",
    "        \n",
    "        return self.path\n",
    "    \n",
    "# download an archive containing the dataset and store it into the output directory\n",
    "url = 'http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz'\n",
    "dl = GoogleDriveDownloader(url, 'data')\n",
    "dl.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading of the compressed file, we extract it and also assess some statistics to verify whether the gathered data consist the expected number of classes and images. Here's an example execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tgz(from_path, to_path=None, img_extention='.jpg'):\n",
    "    \"\"\"\n",
    "    Extracts data from '.tgz' file and displays data statistics.\n",
    "    Returns the output directory name.  \n",
    "    \"\"\"\n",
    "    with tarfile.open(from_path, 'r:gz') as tar:   \n",
    "        \n",
    "        if to_path is None:\n",
    "            out_dir = os.path.splitext(from_path)[0]\n",
    "        if os.path.isdir(out_dir):\n",
    "            print('Files are extracted yet.')\n",
    "        else:\n",
    "            print('Extracting files...')\n",
    "        to_path = os.path.dirname(out_dir)\n",
    "\n",
    "        subdir_and_files = [tarinfo for tarinfo in tar.getmembers()]    \n",
    "        imgs = [t for t in subdir_and_files if t.name.endswith(img_extention)]\n",
    "        print('\\tClasses: {}\\n\\tImages: {}'.format(len(set([os.path.dirname(t.name) for t in imgs])), len(imgs)))\n",
    "\n",
    "        tar.extractall(to_path, members=subdir_and_files)\n",
    "        \n",
    "        return out_dir\n",
    "\n",
    "\n",
    "# extract the downloaded archive & assess data statistics\n",
    "in_dir_data = extract_tgz(from_path=dl.fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Exploratory data analysis is an essential starting point of any data science project, which provides a better data understanding and lays the foundation for the further analysis. First, we are going to make a quality check to ensure files are correctly collected and verify whether the data cleaning will be needed: \n",
    "\n",
    "* Question 1: Do corrupted images exist in our dataset?\n",
    "\n",
    "In this tutorial, we plan to evaluate several network's architectures, including the one which are pre-trained using the ImageNet dataset. Since pre-trained models usually expect input data to be normalized in the same way, such as a height and width are at least 224 pixels, we can start thinking about:\n",
    "\n",
    "* Question 2: What would be the optimal data transformation strategy?\n",
    "\n",
    "Additionally, we plan to estimate the position on which birds more likely to be located on the image. This he solution will be helpful to define the appropriate way for the data augmentation later on. For that, we plan to investigate: \n",
    "\n",
    "* Question 3: Are there any image-specific biases that can limit the model performance?\n",
    "\n",
    "Let's start exploring data from the CUB-200-2011 dataset.  \n",
    "\n",
    "### Question 1: Do corrupted images exist in our dataset?\n",
    "\n",
    "Data cleaning is the process of ensuring data is correct, so we want to assess whether downloaded dataset contains corrupted images. For that purpose, we'll calculate standard deviation among all pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths(path_to_data, fileformat='.jpg'):\n",
    "    \"\"\"\n",
    "    Returns patgs to files of the specified format.\n",
    "    \"\"\"\n",
    "    filepaths = []\n",
    "    for root, _, filenames in os.walk(path_to_data):\n",
    "        for fn in filenames:\n",
    "            if fn.endswith(fileformat):\n",
    "                filepaths.append(os.path.join(root, fn))\n",
    "                \n",
    "    return filepaths\n",
    "\n",
    "def clearning_worker(path_to_img):\n",
    "    \"\"\"\n",
    "    Verifies whether the image in corrupted.\n",
    "    \"\"\"\n",
    "    std = np.std(mpimg.imread(path_to_img))\n",
    "    img_ok = not np.isclose(std, 0.0)\n",
    "    \n",
    "    return img_ok, path_to_img\n",
    "\n",
    "# calculate standard deviation of images\n",
    "#  multiprocessing(mp)\n",
    "imgs_corrupted = []\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    for img_ok, fn in pool.imap_unordered(cleaning_worker, get_filepaths(in_dir_img)):\n",
    "        if not img_ok:\n",
    "            imgs_corrupted.append(fn)\n",
    "\n",
    "# verify do corrupted images (missing data) exist\n",
    "print('Corrupted images #: ', len(imgs_corrupted))\n",
    "\n",
    "# clean up the images that arent' OK\n",
    "for fn in imgs_corrupted:\n",
    "    os.remove(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: What would be an optimal image transformation strategy?\n",
    "\n",
    "In order to define the optimal data transformation strategy, we'll explore the format of images to see what useful we can grasp on. Let’s have a look at some bird examples of the sparrow family:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "in_dir_img = os.path.join(in_dir_data, 'images')\n",
    "\n",
    "# obtain classes of sparrow species\n",
    "img_sparrows = dict()  # {}\n",
    "cls_sparrows_total = [k for k in os.listdir(in_dir_img) if 'sparrow' in k.lower()]\n",
    "\n",
    "# get images of some sparrow classes\n",
    "cls_sparrows = cls_sparrows_total[1::2][:5]\n",
    "for dirname in cls_sparrows:\n",
    "    imgs = []\n",
    "    for dp, _, fn in os.walk(os.path.join(in_dir_img, dirname)):\n",
    "        imgs_extend(fn)\n",
    "    img_sparrows[dirname] = imgs\n",
    "\n",
    "# visualize randomly-chosen images\n",
    "n_cls = len(cls_sparrows)\n",
    "f, ax = plt.subplots(1, n_cls, figsize=(14, 8))\n",
    "\n",
    "for i in range(n_cls):\n",
    "    cls_name = cls_sparrows[random.randint(0, n_cls - 1)]\n",
    "    n_img = len(img_sparrows[cls_name])\n",
    "    img_name = img_sparrows[cls_name][random.randint(0, n_img - 1)]\n",
    "    path_img = os.path.join(os.path.join(in_dir_img, cls_name), img_name)\n",
    "    ax[i].imshow(mpimg.imread(path_img))\n",
    "    ax[i].set_title(cls_name.split('.')[-1].replace('_', ' '), fontsize=12)\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, there can be a high similarity among birds related to different species which is really hard to spot. Is that a Song or a White-throated Sparrow? Well, even experts can be confused...\n",
    "\n",
    "Just out of interest, we'll sum up all classes of the Sparrow family to understand how many of them in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the total number of sparrow species\n",
    "print(len(cls_sparrows_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several dozen different species, represented only by  sparrows. Too much honor ever for one family, isn't it? And now we see why the CUB-200-2011 dataset is perfectly designed for fine-grained classification. What do we have is the many similar birds potentially related to different classes.  \n",
    "\n",
    "To understand how variable the image size is, let's analyze the widths and heights distribution of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate image statistics (takes some time to complete)\n",
    "df = tv.datasets.ImageFolder(in_dir_img)\n",
    "shapes = [(img.height, img.width) for img, _ in ds]\n",
    "heights, widths = [[h for h, _ in shapes], [w for _, w in shapes]]\n",
    "print(\"Average sizes: \", *map*np.median, zip(*shapes))\n",
    "\n",
    "# visualize the distribution of the size of images\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "bp = ax.boxplot([heights, widths], patch_artist=True)\n",
    "\n",
    "ax.set_xticklabels(['height', 'width'])\n",
    "ax.set_xlabel('image size')\n",
    "ax.set_ylabel('pixels')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jipiration",
   "language": "python",
   "name": "jipiration"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
